{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szaravy/.cache/pypoetry/virtualenvs/nlp-pUImhLN--py3.13/lib/python3.13/site-packages/dask/dataframe/__init__.py:49: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "import subprocess\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets download\n",
    "\n",
    "Requires [Kaggle API](https://www.kaggle.com/docs/api#authentication) token in one of these directories:\n",
    "```\n",
    "~/.kaggle/kaggle.json\n",
    "\n",
    "~/.config/kaggle/kaggle.json\n",
    "```\n",
    "\n",
    "Used datasets:\n",
    "- [Main OSes terminal commands](https://www.kaggle.com/datasets/vaibhavdlights/linuxcmdmacos-commands)\n",
    "- [Wikipedia sentences](https://www.kaggle.com/datasets/mikeortman/wikipedia-sentences)\n",
    "- [Wikipedia plaintext 2023](https://www.kaggle.com/datasets/jjinho/wikipedia-20230701)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data storage\n",
    "subprocess.run([\"mkdir\", \"data\"])\n",
    "\n",
    "# CLI commands dataset\n",
    "pathCommands = kagglehub.dataset_download(\"vaibhavdlights/linuxcmdmacos-commands\")\n",
    "subprocess.run([\"mv\", pathCommands, \"./data/commands/\"])\n",
    "\n",
    "# Wikipedia sentences dataset\n",
    "pathWiki = kagglehub.dataset_download(\"mikeortman/wikipedia-sentences\")\n",
    "subprocess.run([\"mv\", pathWiki, \"./data/wikiSen/\"])\n",
    "\n",
    "# Wikipedia plaintext 2023 dataset\n",
    "pathWikiPlain = kagglehub.dataset_download(\"jjinho/wikipedia-20230701\")\n",
    "subprocess.run([\"mv\", pathWikiPlain, \"./data/wikiPlain/\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data manipulation\n",
    "\n",
    "Removing punctuation and formatting the text, joining the separate files into one txt file so it's only plain text ready for unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linuxCommandsDf = pd.read_csv('data/commands/linux_commands.csv')\n",
    "cmdCommandsDf = pd.read_csv('data/commands/cmd_commands.csv')\n",
    "macOsCommandsDf = pd.read_csv('data/commands/macos_commands.csv')\n",
    "vbscriptCommandsDf = pd.read_csv('data/commands/vbscript_commands.csv')\n",
    "\n",
    "\n",
    "commandsDf = pd.concat([linuxCommandsDf, cmdCommandsDf, macOsCommandsDf, vbscriptCommandsDf], ignore_index = True)\n",
    "del linuxCommandsDf\n",
    "del cmdCommandsDf\n",
    "del macOsCommandsDf\n",
    "del vbscriptCommandsDf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining data frames\n",
    "\n",
    "subprocess.run([\"mkdir\", \"data/clean\"])\n",
    "\n",
    "# Commands dataset\n",
    "linuxCommandsDf = pd.read_csv('data/commands/linux_commands.csv')\n",
    "cmdCommandsDf = pd.read_csv('data/commands/cmd_commands.csv')\n",
    "macOsCommandsDf = pd.read_csv('data/commands/macos_commands.csv')\n",
    "vbscriptCommandsDf = pd.read_csv('data/commands/vbscript_commands.csv')\n",
    "\n",
    "\n",
    "commandsDf = pd.concat([linuxCommandsDf, cmdCommandsDf, macOsCommandsDf, vbscriptCommandsDf], ignore_index = True)\n",
    "del linuxCommandsDf\n",
    "del cmdCommandsDf\n",
    "del macOsCommandsDf\n",
    "del vbscriptCommandsDf\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Removing duplicate columns (indexes)\n",
    "commandsDf = commandsDf.drop(columns=['Unnamed: 0'])\n",
    "commandsDf = commandsDf.drop(columns=['description'])\n",
    "\n",
    "# Removing unwanted parts of strings\n",
    "commandsDf['description'] = commandsDf['description'].str.replace(' •', '')\n",
    "commandsDf['description'] = commandsDf['description'].str.replace('•', '')\n",
    "\n",
    "# Saving the data frame to a plain text file\n",
    "commandsDf.to_csv('data/clean/commandsDf.txt', sep='\\t', index=False, header=False)\n",
    "\n",
    "del commandsDf\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# wikiPlain dataset\n",
    "wikiPlainDf = dd.read_parquet('data/wikiPlain/*.parquet')\n",
    "\n",
    "# Processing the wikiPlain dataset in chunks\n",
    "for chunk in wikiPlainDf.to_delayed():\n",
    "    chunk_df = chunk.compute()\n",
    "\n",
    "    chunk_df = chunk_df.drop(columns=['id', 'categories'])\n",
    "\n",
    "    chunk_df.to_csv('data/clean/wikiPlain.txt', sep='\\t', index=False, header=False, mode='a')\n",
    "\n",
    "    del chunk_df\n",
    "    gc.collect()\n",
    "\n",
    "del wikiPlainDf\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Joining all the text files\n",
    "data = data2 = \"\"\n",
    "\n",
    "with open('data/clean/commandsDf.txt') as fileWrite:\n",
    "    data = fileWrite.read()\n",
    "\n",
    "with open('data/wikiSen/wikisent2.txt') as fileWrite:\n",
    "    data2 = fileWrite.read()\n",
    "\n",
    "data += data2\n",
    "\n",
    "with open('data/clean/wikiPlain.txt', 'r') as wikiPlainFile, open('data/clean/dataFull.txt', 'w') as dataFullFile:\n",
    "    # Writing the previous txt files to the full dataset text file\n",
    "    dataFullFile.write(data)\n",
    "\n",
    "    for line in wikiPlainFile:\n",
    "        dataFullFile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word vectors\n",
    "\n",
    "Word representation\n",
    "\n",
    "Data split - 80% training 20% test\n",
    "\n",
    "CBOW vs skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "import fasttext\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 127M words\n",
      "Number of words:  486699\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   52007 lr:  0.000000 avg.loss:  0.485565 ETA:   0h 0m 0s53s  70494 lr:  0.048440 avg.loss:  1.697119 ETA:   0h20m51s  70493 lr:  0.048086 avg.loss:  1.669816 ETA:   0h20m42s  6.6% words/sec/thread:   69553 lr:  0.046714 avg.loss:  1.605509 ETA:   0h20m23s 1.518650 ETA:   0h19m50s 0.045232 avg.loss:  1.483721 ETA:   0h19m42s 10.2% words/sec/thread:   69605 lr:  0.044902 avg.loss:  1.464940 ETA:   0h19m35s lr:  0.043324 avg.loss:  1.413436 ETA:   0h18m55s 13.6% words/sec/thread:   69489 lr:  0.043212 avg.loss:  1.411198 ETA:   0h18m52s 1.410845 ETA:   0h18m52s 13.6% words/sec/thread:   69462 lr:  0.043187 avg.loss:  1.410705 ETA:   0h18m52s 14.2% words/sec/thread:   69289 lr:  0.042906 avg.loss:  1.405271 ETA:   0h18m48s 18.9% words/sec/thread:   67672 lr:  0.040567 avg.loss:  1.279591 ETA:   0h18m12ss 0.040246 avg.loss:  1.247219 ETA:   0h18m 7s 1.210666 ETA:   0h18m 2s 0.039320 avg.loss:  1.162335 ETA:   0h17m56s ETA:   0h17m54s ETA:   0h17m54s 22.6% words/sec/thread:   65976 lr:  0.038703 avg.loss:  1.127338 ETA:   0h17m48s ETA:   0h17m48s  0h17m42s 25.5% words/sec/thread:   64810 lr:  0.037227 avg.loss:  1.066986 ETA:   0h17m26s avg.loss:  1.038939 ETA:   0h17m17s 27.3% words/sec/thread:   64189 lr:  0.036337 avg.loss:  1.018264 ETA:   0h17m11ssh16m31s 33.3% words/sec/thread:   62267 lr:  0.033337 avg.loss:  0.894739 ETA:   0h16m15s 33.4% words/sec/thread:   62241 lr:  0.033289 avg.loss:  0.893769 ETA:   0h16m14s words/sec/thread:   61728 lr:  0.032324 avg.loss:  0.863160 ETA:   0h15m54sh15m40s15m28s 38.6% words/sec/thread:   60921 lr:  0.030711 avg.loss:  0.815399 ETA:   0h15m18s avg.loss:  0.762240 ETA:   0h14m30ss 0.738798 ETA:   0h14m 2s 46.2% words/sec/thread:   59214 lr:  0.026907 avg.loss:  0.727757 ETA:   0h13m47sh13m42s ETA:   0h13m23s ETA:   0h12m49s% words/sec/thread:   58102 lr:  0.023764 avg.loss:  0.676679 ETA:   0h12m25sm53sh11m20sm15s  56930 lr:  0.021044 avg.loss:  0.644420 ETA:   0h11m13s ETA:   0h10m56s ETA:   0h10m44s words/sec/thread:   56321 lr:  0.019037 avg.loss:  0.622608 ETA:   0h10m15s 63.4% words/sec/thread:   56091 lr:  0.018284 avg.loss:  0.618116 ETA:   0h 9m53s lr:  0.017905 avg.loss:  0.615899 ETA:   0h 9m42s 64.4% words/sec/thread:   55961 lr:  0.017811 avg.loss:  0.615349 ETA:   0h 9m39s ETA:   0h 9m37s 0.612368 ETA:   0h 9m24s 66.0% words/sec/thread:   55730 lr:  0.016990 avg.loss:  0.610683 ETA:   0h 9m15s lr:  0.016915 avg.loss:  0.610262 ETA:   0h 9m13s 66.6% words/sec/thread:   55640 lr:  0.016678 avg.loss:  0.608946 ETA:   0h 9m 6s words/sec/thread:   55579 lr:  0.016448 avg.loss:  0.607676 ETA:   0h 8m59s avg.loss:  0.604333 ETA:   0h 8m40s 8m20s 71.3% words/sec/thread:   55063 lr:  0.014347 avg.loss:  0.596554 ETA:   0h 7m54s 71.7% words/sec/thread:   55015 lr:  0.014174 avg.loss:  0.595666 ETA:   0h 7m49s 73.5% words/sec/thread:   54804 lr:  0.013243 avg.loss:  0.591006 ETA:   0h 7m20sm34s 0.581255 ETA:   0h 6m23s lr:  0.011266 avg.loss:  0.580384 ETA:   0h 6m17sm14sh 5m52s 5m32s% words/sec/thread:   53916 lr:  0.009426 avg.loss:  0.568778 ETA:   0h 5m18s 0.559101 ETA:   0h 4m37s avg.loss:  0.554105 ETA:   0h 4m10s avg.loss:  0.553368 ETA:   0h 4m 7s 85.8% words/sec/thread:   53389 lr:  0.007100 avg.loss:  0.552198 ETA:   0h 4m 2ss 88.0% words/sec/thread:   53140 lr:  0.005988 avg.loss:  0.544247 ETA:   0h 3m25s  53088 lr:  0.005747 avg.loss:  0.542587 ETA:   0h 3m17s% words/sec/thread:   53084 lr:  0.005727 avg.loss:  0.542457 ETA:   0h 3m16s  53080 lr:  0.005707 avg.loss:  0.542322 ETA:   0h 3m15s avg.loss:  0.536707 ETA:   0h 2m46s 90.4% words/sec/thread:   52907 lr:  0.004788 avg.loss:  0.536353 ETA:   0h 2m44ss 92.9% words/sec/thread:   52646 lr:  0.003528 avg.loss:  0.521187 ETA:   0h 2m 2sm 1s% words/sec/thread:   52619 lr:  0.003404 avg.loss:  0.520222 ETA:   0h 1m57s% words/sec/thread:   52599 lr:  0.003296 avg.loss:  0.518784 ETA:   0h 1m54s 0.003119 avg.loss:  0.516444 ETA:   0h 1m48s avg.loss:  0.515993 ETA:   0h 1m46s 94.5% words/sec/thread:   52491 lr:  0.002742 avg.loss:  0.511570 ETA:   0h 1m35s  52457 lr:  0.002544 avg.loss:  0.509016 ETA:   0h 1m28s 0.002130 avg.loss:  0.503749 ETA:   0h 1m14s avg.loss:  0.492546 ETA:   0h 0m37s 0m31s% words/sec/thread:   52106 lr:  0.000629 avg.loss:  0.489686 ETA:   0h 0m21s words/sec/thread:   52062 lr:  0.000366 avg.loss:  0.487864 ETA:   0h 0m12s 99.3% words/sec/thread:   52058 lr:  0.000358 avg.loss:  0.487787 ETA:   0h 0m12s 99.3% words/sec/thread:   52052 lr:  0.000334 avg.loss:  0.487648 ETA:   0h 0m11s lr:  0.000033 avg.loss:  0.485772 ETA:   0h 0m 1s\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "\n",
    "with open('data/data.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Saving the split data to new files\n",
    "with open('data/train_data.txt', 'w') as train_file:\n",
    "    train_file.writelines(train_data)\n",
    "\n",
    "with open('data/test_data.txt', 'w') as test_file:\n",
    "    test_file.writelines(test_data)\n",
    "\n",
    "# TODO zostawic commandsDf.txt w train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 127M words\n",
      "Number of words:  486699\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   99146 lr:  0.000000 avg.loss:  1.046694 ETA:   0h 0m 0s lr:  0.047926 avg.loss:  1.859780 ETA:   0h 9m40s 9m27s words/sec/thread:  149067 lr:  0.046768 avg.loss:  1.750912 ETA:   0h 9m31s32s 16.2% words/sec/thread:  145456 lr:  0.041901 avg.loss:  1.568963 ETA:   0h 8m44s avg.loss:  1.555797 ETA:   0h 8m44s% words/sec/thread:  134246 lr:  0.038837 avg.loss:  1.503370 ETA:   0h 8m47s 0.038825 avg.loss:  1.502928 ETA:   0h 8m47s avg.loss:  1.473101 ETA:   0h 8m46s% words/sec/thread:  123724 lr:  0.034045 avg.loss:  1.394181 ETA:   0h 8m21s words/sec/thread:  122907 lr:  0.033567 avg.loss:  1.385990 ETA:   0h 8m17s 122863 lr:  0.033545 avg.loss:  1.385620 ETA:   0h 8m17s 121384 lr:  0.032575 avg.loss:  1.370889 ETA:   0h 8m 8s 41.5% words/sec/thread:  117372 lr:  0.029262 avg.loss:  1.327538 ETA:   0h 7m34s 0.029032 avg.loss:  1.321859 ETA:   0h 7m32s22s ETA:   0h 7m19s% words/sec/thread:  113714 lr:  0.026238 avg.loss:  1.260349 ETA:   0h 7m 0s avg.loss:  1.257471 ETA:   0h 6m57s 113204 lr:  0.025695 avg.loss:  1.254839 ETA:   0h 6m53s 1.240142 ETA:   0h 6m26s 56.4% words/sec/thread:  109972 lr:  0.021819 avg.loss:  1.227172 ETA:   0h 6m 1s words/sec/thread:  108695 lr:  0.019942 avg.loss:  1.197230 ETA:   0h 5m34s% words/sec/thread:  107545 lr:  0.018140 avg.loss:  1.175120 ETA:   0h 5m 7s 1.151955 ETA:   0h 4m26s 0.014834 avg.loss:  1.148549 ETA:   0h 4m15s 72.4% words/sec/thread:  105232 lr:  0.013824 avg.loss:  1.143487 ETA:   0h 3m59s% words/sec/thread:  105205 lr:  0.013769 avg.loss:  1.143209 ETA:   0h 3m58s 73.8% words/sec/thread:  104888 lr:  0.013120 avg.loss:  1.140018 ETA:   0h 3m47s lr:  0.011718 avg.loss:  1.133122 ETA:   0h 3m24s19ss 91.5% words/sec/thread:  101314 lr:  0.004270 avg.loss:  1.079575 ETA:   0h 1m16s 1.073781 ETA:   0h 1m 2s 0m49s 0m41s 95.7% words/sec/thread:  100285 lr:  0.002154 avg.loss:  1.063724 ETA:   0h 0m39s% words/sec/thread:   99491 lr:  0.000564 avg.loss:  1.051732 ETA:   0h 0m10sm 6s words/sec/thread:   99288 lr:  0.000258 avg.loss:  1.049488 ETA:   0h 0m 4ss% words/sec/thread:   99208 lr:  0.000123 avg.loss:  1.048078 ETA:   0h 0m 2s\n",
      "Read 127M words\n",
      "Number of words:  486699\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   47848 lr:  0.000000 avg.loss:  0.522624 ETA:   0h 0m 0s avg.loss:  4.110000 ETA:   0h27m22s  0.4% words/sec/thread:   55928 lr:  0.049809 avg.loss:  2.073396 ETA:   0h27m 2s13s words/sec/thread:   53016 lr:  0.047823 avg.loss:  1.669444 ETA:   0h27m23sh27m29s words/sec/thread:   52099 lr:  0.047273 avg.loss:  1.647591 ETA:   0h27m33s% words/sec/thread:   51496 lr:  0.046690 avg.loss:  1.614091 ETA:   0h27m31s27m32s ETA:   0h27m24s% words/sec/thread:   51091 lr:  0.046008 avg.loss:  1.590055 ETA:   0h27m20ssm10s  9.5% words/sec/thread:   50659 lr:  0.045261 avg.loss:  1.539399 ETA:   0h27m 7s words/sec/thread:   50582 lr:  0.044830 avg.loss:  1.519027 ETA:   0h26m54s26m30s lr:  0.043628 avg.loss:  1.462606 ETA:   0h26m24s 13.3% words/sec/thread:   50085 lr:  0.043338 avg.loss:  1.453872 ETA:   0h26m16s% words/sec/thread:   50140 lr:  0.042976 avg.loss:  1.439367 ETA:   0h26m 1s 1.433340 ETA:   0h25m54s words/sec/thread:   50208 lr:  0.042760 avg.loss:  1.430471 ETA:   0h25m51s 0.042567 avg.loss:  1.425419 ETA:   0h25m43s  0h25m13s lr:  0.041330 avg.loss:  1.380924 ETA:   0h24m50s 0.041194 avg.loss:  1.376441 ETA:   0h24m44s  50534 lr:  0.040324 avg.loss:  1.354543 ETA:   0h24m13s% words/sec/thread:   50527 lr:  0.040284 avg.loss:  1.353963 ETA:   0h24m12s 19.7% words/sec/thread:   50542 lr:  0.040172 avg.loss:  1.352491 ETA:   0h24m 8s24m 6s% words/sec/thread:   50709 lr:  0.039392 avg.loss:  1.282533 ETA:   0h23m35s 22.9% words/sec/thread:   50826 lr:  0.038569 avg.loss:  1.201008 ETA:   0h23m 2s 1.155733 ETA:   0h22m28s  50686 lr:  0.037208 avg.loss:  1.128333 ETA:   0h22m17s 26.4% words/sec/thread:   50648 lr:  0.036790 avg.loss:  1.098614 ETA:   0h22m 3sh21m59s 26.7% words/sec/thread:   50636 lr:  0.036673 avg.loss:  1.091020 ETA:   0h21m59s 26.7% words/sec/thread:   50635 lr:  0.036667 avg.loss:  1.090829 ETA:   0h21m59s 28.6% words/sec/thread:   50700 lr:  0.035725 avg.loss:  1.051851 ETA:   0h21m23s 29.2% words/sec/thread:   50712 lr:  0.035381 avg.loss:  1.036095 ETA:   0h21m11s lr:  0.034998 avg.loss:  1.014108 ETA:   0h20m57s lr:  0.034577 avg.loss:  0.992216 ETA:   0h20m42s lr:  0.032516 avg.loss:  0.912222 ETA:   0h19m34sm26s  50306 lr:  0.031679 avg.loss:  0.893365 ETA:   0h19m 7s 0.892936 ETA:   0h19m 6s 36.9% words/sec/thread:   50290 lr:  0.031570 avg.loss:  0.891220 ETA:   0h19m 3s% words/sec/thread:   50281 lr:  0.031521 avg.loss:  0.890234 ETA:   0h19m 2s 0.887628 ETA:   0h18m57s ETA:   0h18m56s18m55s ETA:   0h18m45ss 39.3% words/sec/thread:   50045 lr:  0.030361 avg.loss:  0.864361 ETA:   0h18m25s words/sec/thread:   50036 lr:  0.030137 avg.loss:  0.857050 ETA:   0h18m17s avg.loss:  0.838707 ETA:   0h17m50s words/sec/thread:   50099 lr:  0.029342 avg.loss:  0.837216 ETA:   0h17m47s% words/sec/thread:   50150 lr:  0.028781 avg.loss:  0.828595 ETA:   0h17m25s 42.6% words/sec/thread:   50159 lr:  0.028682 avg.loss:  0.827108 ETA:   0h17m21s 43.2% words/sec/thread:   50160 lr:  0.028421 avg.loss:  0.823193 ETA:   0h17m12s 0.817224 ETA:   0h17m 3s avg.loss:  0.810374 ETA:   0h16m54ss 45.8% words/sec/thread:   49912 lr:  0.027124 avg.loss:  0.790809 ETA:   0h16m30s 47.0% words/sec/thread:   49875 lr:  0.026487 avg.loss:  0.776404 ETA:   0h16m 7s  0h15m54s  49741 lr:  0.025639 avg.loss:  0.759219 ETA:   0h15m39s 49.7% words/sec/thread:   49635 lr:  0.025147 avg.loss:  0.751842 ETA:   0h15m23s words/sec/thread:   49632 lr:  0.025110 avg.loss:  0.751449 ETA:   0h15m21s 0.751339 ETA:   0h15m21ss 51.6% words/sec/thread:   49547 lr:  0.024179 avg.loss:  0.737107 ETA:   0h14m49s ETA:   0h14m29sh14m28s 0.023469 avg.loss:  0.725238 ETA:   0h14m24s 0.723043 ETA:   0h14m20s% words/sec/thread:   49392 lr:  0.022882 avg.loss:  0.718154 ETA:   0h14m 4s 57.4% words/sec/thread:   49189 lr:  0.021313 avg.loss:  0.697869 ETA:   0h13m 9s 58.2% words/sec/thread:   49164 lr:  0.020882 avg.loss:  0.694225 ETA:   0h12m53s lr:  0.020757 avg.loss:  0.693157 ETA:   0h12m49s 58.9% words/sec/thread:   49127 lr:  0.020545 avg.loss:  0.690283 ETA:   0h12m41s 0.020504 avg.loss:  0.689672 ETA:   0h12m40s 0.681981 ETA:   0h12m20s 62.3% words/sec/thread:   49062 lr:  0.018832 avg.loss:  0.667702 ETA:   0h11m39s% words/sec/thread:   48972 lr:  0.018539 avg.loss:  0.663564 ETA:   0h11m29s words/sec/thread:   48923 lr:  0.018364 avg.loss:  0.661167 ETA:   0h11m23s words/sec/thread:   48849 lr:  0.018200 avg.loss:  0.659676 ETA:   0h11m18s 64.0% words/sec/thread:   48795 lr:  0.018013 avg.loss:  0.658089 ETA:   0h11m12s% words/sec/thread:   48698 lr:  0.017711 avg.loss:  0.655159 ETA:   0h11m 2s  48683 lr:  0.017696 avg.loss:  0.654988 ETA:   0h11m 2s avg.loss:  0.654089 ETA:   0h10m59s 64.8% words/sec/thread:   48619 lr:  0.017596 avg.loss:  0.654056 ETA:   0h10m59s% words/sec/thread:   48585 lr:  0.017497 avg.loss:  0.653327 ETA:   0h10m56s 65.0% words/sec/thread:   48582 lr:  0.017488 avg.loss:  0.653272 ETA:   0h10m55s 65.1% words/sec/thread:   48580 lr:  0.017471 avg.loss:  0.653153 ETA:   0h10m55s% words/sec/thread:   48570 lr:  0.016454 avg.loss:  0.646299 ETA:   0h10m17sh10m10s 0.016207 avg.loss:  0.643175 ETA:   0h10m 8ss  48412 lr:  0.014491 avg.loss:  0.626563 ETA:   0h 9m 5s 0.013354 avg.loss:  0.617473 ETA:   0h 8m22s% words/sec/thread:   48445 lr:  0.012740 avg.loss:  0.614117 ETA:   0h 7m59s lr:  0.012571 avg.loss:  0.613143 ETA:   0h 7m52s 0.012014 avg.loss:  0.609033 ETA:   0h 7m32s 0.011299 avg.loss:  0.603482 ETA:   0h 7m 5s lr:  0.010572 avg.loss:  0.596922 ETA:   0h 6m38s avg.loss:  0.596023 ETA:   0h 6m31s 79.3% words/sec/thread:   48325 lr:  0.010373 avg.loss:  0.595847 ETA:   0h 6m31s words/sec/thread:   48326 lr:  0.010082 avg.loss:  0.594363 ETA:   0h 6m20s words/sec/thread:   48326 lr:  0.009982 avg.loss:  0.593901 ETA:   0h 6m16s 80.2% words/sec/thread:   48322 lr:  0.009903 avg.loss:  0.593508 ETA:   0h 6m13s 6m11s 81.6% words/sec/thread:   48277 lr:  0.009191 avg.loss:  0.589428 ETA:   0h 5m46s  0h 5m15s 0.580389 ETA:   0h 4m59s words/sec/thread:   48165 lr:  0.007752 avg.loss:  0.578853 ETA:   0h 4m53s 4m30s  48098 lr:  0.007128 avg.loss:  0.574283 ETA:   0h 4m29s 0.006939 avg.loss:  0.572936 ETA:   0h 4m22s% words/sec/thread:   48084 lr:  0.006849 avg.loss:  0.572198 ETA:   0h 4m19s 87.4% words/sec/thread:   48066 lr:  0.006320 avg.loss:  0.567880 ETA:   0h 3m59s 87.5% words/sec/thread:   48068 lr:  0.006241 avg.loss:  0.567247 ETA:   0h 3m56s 88.9% words/sec/thread:   48058 lr:  0.005549 avg.loss:  0.562004 ETA:   0h 3m30s 89.1% words/sec/thread:   48040 lr:  0.005434 avg.loss:  0.561508 ETA:   0h 3m26s ETA:   0h 3m24s% words/sec/thread:   48022 lr:  0.005112 avg.loss:  0.559878 ETA:   0h 3m13s 90.4% words/sec/thread:   48006 lr:  0.004807 avg.loss:  0.557994 ETA:   0h 3m 2s 90.5% words/sec/thread:   47998 lr:  0.004735 avg.loss:  0.557700 ETA:   0h 2m59sh 2m45s 0.552445 ETA:   0h 2m12s56s% words/sec/thread:   47951 lr:  0.003004 avg.loss:  0.550608 ETA:   0h 1m54ss ETA:   0h 0m58s 0.001199 avg.loss:  0.537575 ETA:   0h 0m45s ETA:   0h 0m31sh 0m31s\n",
      "mkdir: cannot create directory ‘result’: File exists\n"
     ]
    }
   ],
   "source": [
    "# modelDef = fasttext.train_unsupervised('data/train_data.txt')\n",
    "# modelCbow = fasttext.train_unsupervised('data/train_data.txt', 'cbow')\n",
    "# modelSkipgram = fasttext.train_unsupervised('data/train_data.txt', 'skipgram')\n",
    "\n",
    "# Saving different model versions to binary file\n",
    "subprocess.run([\"mkdir\", \"result\"])\n",
    "\n",
    "# modelDef.save_model(\"result/model1.bin\")\n",
    "modelCbow.save_model(\"result/modelCbow.bin\")\n",
    "modelSkipgram.save_model(\"result/modelSkipgram.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 159M words\n",
      "Number of words:  570697\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   97950 lr:  0.000000 avg.loss:  0.935774 ETA:   0h 0m 0s 13.6% words/sec/thread:  153391 lr:  0.043181 avg.loss:  1.432496 ETA:   0h10m41s 14.6% words/sec/thread:  151756 lr:  0.042692 avg.loss:  1.423745 ETA:   0h10m40s words/sec/thread:  150811 lr:  0.042458 avg.loss:  1.417306 ETA:   0h10m41s words/sec/thread:  148730 lr:  0.041952 avg.loss:  1.397673 ETA:   0h10m42s lr:  0.040701 avg.loss:  1.364990 ETA:   0h10m42s words/sec/thread:  144092 lr:  0.040676 avg.loss:  1.364731 ETA:   0h10m42s10m43s 0.039385 avg.loss:  1.338430 ETA:   0h10m47sh10m41s 35.7% words/sec/thread:  121680 lr:  0.032137 avg.loss:  1.214525 ETA:   0h10m 1sh 9m59s 36.5% words/sec/thread:  120972 lr:  0.031733 avg.loss:  1.208570 ETA:   0h 9m57s 119215 lr:  0.030757 avg.loss:  1.196144 ETA:   0h 9m47ss 40.4% words/sec/thread:  117932 lr:  0.029808 avg.loss:  1.190069 ETA:   0h 9m35s lr:  0.029716 avg.loss:  1.189825 ETA:   0h 9m34s 40.6% words/sec/thread:  117800 lr:  0.029684 avg.loss:  1.189765 ETA:   0h 9m33s 41.1% words/sec/thread:  117495 lr:  0.029461 avg.loss:  1.188779 ETA:   0h 9m30s% words/sec/thread:  116689 lr:  0.028917 avg.loss:  1.185208 ETA:   0h 9m24s  0h 9m19s ETA:   0h 9m10s 44.9% words/sec/thread:  114988 lr:  0.027574 avg.loss:  1.175575 ETA:   0h 9m 6s avg.loss:  1.174239 ETA:   0h 9m 2s 46.1% words/sec/thread:  114347 lr:  0.026956 avg.loss:  1.171759 ETA:   0h 8m56s 113926 lr:  0.026563 avg.loss:  1.169279 ETA:   0h 8m50s avg.loss:  1.167876 ETA:   0h 8m47s 48.1% words/sec/thread:  113408 lr:  0.025933 avg.loss:  1.164331 ETA:   0h 8m40sm33s 50.3% words/sec/thread:  112347 lr:  0.024868 avg.loss:  1.152547 ETA:   0h 8m24s% words/sec/thread:  112229 lr:  0.024741 avg.loss:  1.150637 ETA:   0h 8m21s% words/sec/thread:  111055 lr:  0.023603 avg.loss:  1.135984 ETA:   0h 8m 3s 111035 lr:  0.023583 avg.loss:  1.135778 ETA:   0h 8m 3s% words/sec/thread:  110763 lr:  0.023295 avg.loss:  1.134327 ETA:   0h 7m58s avg.loss:  1.131788 ETA:   0h 7m52s 109240 lr:  0.021384 avg.loss:  1.112642 ETA:   0h 7m25s words/sec/thread:  108837 lr:  0.020879 avg.loss:  1.109256 ETA:   0h 7m16ss% words/sec/thread:  107813 lr:  0.019719 avg.loss:  1.101343 ETA:   0h 6m56s 6m46s 62.1% words/sec/thread:  107182 lr:  0.018953 avg.loss:  1.096498 ETA:   0h 6m42s% words/sec/thread:  107068 lr:  0.018816 avg.loss:  1.096025 ETA:   0h 6m40s 6m37s 0.018479 avg.loss:  1.094315 ETA:   0h 6m34s 64.3% words/sec/thread:  106315 lr:  0.017851 avg.loss:  1.090663 ETA:   0h 6m22s words/sec/thread:  106208 lr:  0.017701 avg.loss:  1.089689 ETA:   0h 6m19s 6m10sh 6m 8s 67.8% words/sec/thread:  105231 lr:  0.016117 avg.loss:  1.079940 ETA:   0h 5m48s 1.077976 ETA:   0h 5m41s words/sec/thread:  104957 lr:  0.015658 avg.loss:  1.077230 ETA:   0h 5m39s% words/sec/thread:  104849 lr:  0.015526 avg.loss:  1.076744 ETA:   0h 5m37s 71.2% words/sec/thread:  104169 lr:  0.014400 avg.loss:  1.062357 ETA:   0h 5m14ss 1.038648 ETA:   0h 4m27s 75.9% words/sec/thread:  103030 lr:  0.012032 avg.loss:  1.037794 ETA:   0h 4m25s 78.1% words/sec/thread:  102512 lr:  0.010959 avg.loss:  1.026991 ETA:   0h 4m 3s 0.010212 avg.loss:  1.019338 ETA:   0h 3m47s  0h 3m46s 101794 lr:  0.009487 avg.loss:  1.011446 ETA:   0h 3m32s 101276 lr:  0.008416 avg.loss:  1.000194 ETA:   0h 3m 9s 3m 4s 84.0% words/sec/thread:  101082 lr:  0.007986 avg.loss:  0.996727 ETA:   0h 2m59s 101042 lr:  0.007900 avg.loss:  0.995885 ETA:   0h 2m58s 84.2% words/sec/thread:  101035 lr:  0.007885 avg.loss:  0.995774 ETA:   0h 2m57s avg.loss:  0.995559 ETA:   0h 2m57ss  0h 2m36s 0.006646 avg.loss:  0.982455 ETA:   0h 2m30s 100308 lr:  0.006337 avg.loss:  0.977385 ETA:   0h 2m23s% words/sec/thread:  100171 lr:  0.006016 avg.loss:  0.974640 ETA:   0h 2m16s words/sec/thread:  100056 lr:  0.005682 avg.loss:  0.971910 ETA:   0h 2m 9s  99990 lr:  0.005530 avg.loss:  0.970938 ETA:   0h 2m 5s 89.8% words/sec/thread:   99841 lr:  0.005093 avg.loss:  0.969545 ETA:   0h 1m56s avg.loss:  0.967188 ETA:   0h 1m39s 1m35s  0h 1m20s 93.5% words/sec/thread:   99141 lr:  0.003240 avg.loss:  0.963409 ETA:   0h 1m14s avg.loss:  0.961544 ETA:   0h 1m 1s words/sec/thread:   98889 lr:  0.002559 avg.loss:  0.961176 ETA:   0h 0m58sh 0m22s17s avg.loss:  0.943631 ETA:   0h 0m12s\n",
      "Read 159M words\n",
      "Number of words:  570697\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   46991 lr:  0.000000 avg.loss:  0.391471 ETA:   0h 0m 0s  0.1% words/sec/thread:   68064 lr:  0.049955 avg.loss:  2.034075 ETA:   0h27m51s 0.049611 avg.loss:  1.406065 ETA:   0h31m45s  58295 lr:  0.049419 avg.loss:  1.435136 ETA:   0h32m10s15s words/sec/thread:   52913 lr:  0.048038 avg.loss:  1.485630 ETA:   0h34m27s  5.8% words/sec/thread:   52054 lr:  0.047114 avg.loss:  1.445187 ETA:   0h34m21s  0h34m16s 0.046666 avg.loss:  1.441470 ETA:   0h34m11s% words/sec/thread:   51310 lr:  0.046100 avg.loss:  1.383893 ETA:   0h34m 5s  8.8% words/sec/thread:   50930 lr:  0.045619 avg.loss:  1.348804 ETA:   0h33m59s words/sec/thread:   50833 lr:  0.045484 avg.loss:  1.340063 ETA:   0h33m57s51s 10.0% words/sec/thread:   50621 lr:  0.045006 avg.loss:  1.307790 ETA:   0h33m44ss  50383 lr:  0.044024 avg.loss:  1.264698 ETA:   0h33m 9s% words/sec/thread:   50286 lr:  0.043634 avg.loss:  1.258622 ETA:   0h32m55s 13.4% words/sec/thread:   50334 lr:  0.043298 avg.loss:  1.253339 ETA:   0h32m38s32s 0.041867 avg.loss:  1.232242 ETA:   0h31m38s 1.231389 ETA:   0h31m26sh31m 2s% words/sec/thread:   50128 lr:  0.040731 avg.loss:  1.179331 ETA:   0h30m50s 19.3% words/sec/thread:   50149 lr:  0.040355 avg.loss:  1.142789 ETA:   0h30m32sm25s avg.loss:  1.097831 ETA:   0h30m 7s30m 0s 1.082165 ETA:   0h29m54s 21.5% words/sec/thread:   49895 lr:  0.039234 avg.loss:  1.077710 ETA:   0h29m50s 21.8% words/sec/thread:   49865 lr:  0.039082 avg.loss:  1.065998 ETA:   0h29m44s 1.050629 ETA:   0h29m36s 0.036127 avg.loss:  0.879574 ETA:   0h27m50s  49234 lr:  0.036090 avg.loss:  0.877854 ETA:   0h27m49ss 28.6% words/sec/thread:   49166 lr:  0.035680 avg.loss:  0.858876 ETA:   0h27m32s words/sec/thread:   49132 lr:  0.035508 avg.loss:  0.851236 ETA:   0h27m25s 29.9% words/sec/thread:   49047 lr:  0.035043 avg.loss:  0.830834 ETA:   0h27m 6s ETA:   0h27m 5s 30.1% words/sec/thread:   49021 lr:  0.034930 avg.loss:  0.826134 ETA:   0h27m 2s words/sec/thread:   48999 lr:  0.034786 avg.loss:  0.820057 ETA:   0h26m56s words/sec/thread:   48966 lr:  0.034420 avg.loss:  0.806773 ETA:   0h26m40s ETA:   0h26m30s 0.776455 ETA:   0h25m53s 33.5% words/sec/thread:   48921 lr:  0.033240 avg.loss:  0.774514 ETA:   0h25m47s 33.6% words/sec/thread:   48918 lr:  0.033191 avg.loss:  0.773717 ETA:   0h25m45s% words/sec/thread:   48881 lr:  0.032945 avg.loss:  0.770189 ETA:   0h25m34s avg.loss:  0.770019 ETA:   0h25m34s words/sec/thread:   48877 lr:  0.032897 avg.loss:  0.769370 ETA:   0h25m32s 34.3% words/sec/thread:   48876 lr:  0.032841 avg.loss:  0.768491 ETA:   0h25m30s 34.3% words/sec/thread:   48877 lr:  0.032830 avg.loss:  0.768306 ETA:   0h25m29s 35.4% words/sec/thread:   48852 lr:  0.032308 avg.loss:  0.759884 ETA:   0h25m 5s 35.4% words/sec/thread:   48852 lr:  0.032278 avg.loss:  0.759188 ETA:   0h25m 4s 35.5% words/sec/thread:   48851 lr:  0.032274 avg.loss:  0.759084 ETA:   0h25m 4s% words/sec/thread:   48825 lr:  0.031906 avg.loss:  0.751384 ETA:   0h24m48s% words/sec/thread:   48831 lr:  0.031813 avg.loss:  0.748866 ETA:   0h24m43sm18s 0.031098 avg.loss:  0.732762 ETA:   0h24m10s avg.loss:  0.732127 ETA:   0h24m 8s37s lr:  0.030313 avg.loss:  0.720953 ETA:   0h23m35s 39.7% words/sec/thread:   48767 lr:  0.030153 avg.loss:  0.717564 ETA:   0h23m27s 40.2% words/sec/thread:   48744 lr:  0.029904 avg.loss:  0.712330 ETA:   0h23m17s 40.5% words/sec/thread:   48735 lr:  0.029729 avg.loss:  0.708495 ETA:   0h23m 9s lr:  0.029306 avg.loss:  0.700220 ETA:   0h22m50s22m43s 44.5% words/sec/thread:   48596 lr:  0.027769 avg.loss:  0.674073 ETA:   0h21m41s 45.8% words/sec/thread:   48504 lr:  0.027096 avg.loss:  0.662928 ETA:   0h21m12s 46.7% words/sec/thread:   48443 lr:  0.026647 avg.loss:  0.655977 ETA:   0h20m52s  0h20m52sh20m51s words/sec/thread:   48435 lr:  0.026600 avg.loss:  0.655217 ETA:   0h20m50s words/sec/thread:   48426 lr:  0.026557 avg.loss:  0.654506 ETA:   0h20m48s lr:  0.026466 avg.loss:  0.653079 ETA:   0h20m44s ETA:   0h20m36s words/sec/thread:   48371 lr:  0.026252 avg.loss:  0.650229 ETA:   0h20m35s 48.5% words/sec/thread:   48277 lr:  0.025760 avg.loss:  0.642873 ETA:   0h20m15s% words/sec/thread:   48259 lr:  0.025681 avg.loss:  0.641642 ETA:   0h20m11s avg.loss:  0.641572 ETA:   0h20m11s% words/sec/thread:   48227 lr:  0.025542 avg.loss:  0.639924 ETA:   0h20m 6s 49.0% words/sec/thread:   48226 lr:  0.025525 avg.loss:  0.639798 ETA:   0h20m 5s 0.024832 avg.loss:  0.633733 ETA:   0h19m33s  48253 lr:  0.023836 avg.loss:  0.621086 ETA:   0h18m44sh18m42s  48252 lr:  0.022866 avg.loss:  0.606621 ETA:   0h17m59s words/sec/thread:   48204 lr:  0.022183 avg.loss:  0.597650 ETA:   0h17m27s 0.021986 avg.loss:  0.596339 ETA:   0h17m18s 56.3% words/sec/thread:   48193 lr:  0.021848 avg.loss:  0.595264 ETA:   0h17m12s% words/sec/thread:   48188 lr:  0.021447 avg.loss:  0.590678 ETA:   0h16m53s% words/sec/thread:   48186 lr:  0.021325 avg.loss:  0.589154 ETA:   0h16m47s 0.588218 ETA:   0h16m45s lr:  0.021191 avg.loss:  0.586795 ETA:   0h16m41s% words/sec/thread:   48123 lr:  0.020051 avg.loss:  0.574245 ETA:   0h15m48sh15m28s avg.loss:  0.564884 ETA:   0h15m 3s lr:  0.018777 avg.loss:  0.561932 ETA:   0h14m49s 63.0% words/sec/thread:   48070 lr:  0.018523 avg.loss:  0.559451 ETA:   0h14m37s 63.1% words/sec/thread:   48066 lr:  0.018452 avg.loss:  0.558711 ETA:   0h14m34ss 0.545162 ETA:   0h13m31s 66.3% words/sec/thread:   47951 lr:  0.016875 avg.loss:  0.543242 ETA:   0h13m21s% words/sec/thread:   47929 lr:  0.016658 avg.loss:  0.541271 ETA:   0h13m11s words/sec/thread:   47894 lr:  0.016301 avg.loss:  0.538007 ETA:   0h12m55s 68.4% words/sec/thread:   47861 lr:  0.015781 avg.loss:  0.533689 ETA:   0h12m30s  47860 lr:  0.015723 avg.loss:  0.533152 ETA:   0h12m28s 69.2% words/sec/thread:   47827 lr:  0.015417 avg.loss:  0.530744 ETA:   0h12m14s lr:  0.014373 avg.loss:  0.526251 ETA:   0h11m26s 72.3% words/sec/thread:   47648 lr:  0.013829 avg.loss:  0.523237 ETA:   0h11m 0s ETA:   0h10m55s words/sec/thread:   47640 lr:  0.013628 avg.loss:  0.521673 ETA:   0h10m51s 73.1% words/sec/thread:   47617 lr:  0.013460 avg.loss:  0.520536 ETA:   0h10m43s  47615 lr:  0.013443 avg.loss:  0.520464 ETA:   0h10m42s% words/sec/thread:   47595 lr:  0.013274 avg.loss:  0.519799 ETA:   0h10m35s 0.012678 avg.loss:  0.515444 ETA:   0h10m 8s10m 7s lr:  0.012327 avg.loss:  0.509924 ETA:   0h 9m51sh 9m46s 76.4% words/sec/thread:   47415 lr:  0.011787 avg.loss:  0.503622 ETA:   0h 9m26s avg.loss:  0.502845 ETA:   0h 9m21s words/sec/thread:   47415 lr:  0.011182 avg.loss:  0.498861 ETA:   0h 8m57s 0.010946 avg.loss:  0.497023 ETA:   0h 8m45s 0.493213 ETA:   0h 8m30s 78.9% words/sec/thread:   47413 lr:  0.010553 avg.loss:  0.492227 ETA:   0h 8m26s words/sec/thread:   47403 lr:  0.009918 avg.loss:  0.486810 ETA:   0h 7m56s 80.2% words/sec/thread:   47400 lr:  0.009876 avg.loss:  0.486183 ETA:   0h 7m54s ETA:   0h 7m51s lr:  0.009745 avg.loss:  0.484354 ETA:   0h 7m48s% words/sec/thread:   47384 lr:  0.009004 avg.loss:  0.475601 ETA:   0h 7m12ss 0.008273 avg.loss:  0.467860 ETA:   0h 6m37s% words/sec/thread:   47359 lr:  0.008180 avg.loss:  0.467209 ETA:   0h 6m33ss lr:  0.007994 avg.loss:  0.465339 ETA:   0h 6m24s  47322 lr:  0.007925 avg.loss:  0.464440 ETA:   0h 6m21sm 4s% words/sec/thread:   47290 lr:  0.007128 avg.loss:  0.454392 ETA:   0h 5m43s ETA:   0h 5m34s16s 87.6% words/sec/thread:   47211 lr:  0.006212 avg.loss:  0.443277 ETA:   0h 4m59s words/sec/thread:   47211 lr:  0.006209 avg.loss:  0.443250 ETA:   0h 4m59s lr:  0.006063 avg.loss:  0.442169 ETA:   0h 4m52s  47202 lr:  0.006046 avg.loss:  0.441963 ETA:   0h 4m51s 88.5% words/sec/thread:   47184 lr:  0.005726 avg.loss:  0.439291 ETA:   0h 4m36s 88.8% words/sec/thread:   47174 lr:  0.005594 avg.loss:  0.438454 ETA:   0h 4m30s 0.438271 ETA:   0h 4m28s words/sec/thread:   47164 lr:  0.005445 avg.loss:  0.437492 ETA:   0h 4m22sm14s 3m53s  47123 lr:  0.004806 avg.loss:  0.430209 ETA:   0h 3m52s% words/sec/thread:   47106 lr:  0.004494 avg.loss:  0.426563 ETA:   0h 3m37s  47083 lr:  0.004019 avg.loss:  0.421299 ETA:   0h 3m14s 92.2% words/sec/thread:   47075 lr:  0.003898 avg.loss:  0.419943 ETA:   0h 3m 8s  47062 lr:  0.003594 avg.loss:  0.416704 ETA:   0h 2m53s avg.loss:  0.415164 ETA:   0h 2m40s words/sec/thread:   47050 lr:  0.003140 avg.loss:  0.414240 ETA:   0h 2m31s% words/sec/thread:   47043 lr:  0.002728 avg.loss:  0.411754 ETA:   0h 2m12s  47038 lr:  0.002624 avg.loss:  0.410842 ETA:   0h 2m 7s 95.2% words/sec/thread:   47031 lr:  0.002387 avg.loss:  0.408614 ETA:   0h 1m55s ETA:   0h 1m 6s 0.394690 ETA:   0h 0m31s 0.000503 avg.loss:  0.393926 ETA:   0h 0m24s\n"
     ]
    }
   ],
   "source": [
    "# Full dataset training\n",
    "\n",
    "modelCbowFull = fasttext.train_unsupervised('data/data.txt', 'cbow')\n",
    "modelSkipgramFull = fasttext.train_unsupervised('data/data.txt', 'skipgram')\n",
    "\n",
    "modelCbowFull.save_model('result/modelCbowFull.bin')\n",
    "modelSkipgramFull.save_model('result/modelSkipgramFull.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  202\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   49649 lr:  0.000000 avg.loss:  4.123011 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# CommandsDf dataset training\n",
    "\n",
    "modelSkipgramCommands = fasttext.train_unsupervised('data/commandsDf.txt', 'skipgram')\n",
    "\n",
    "modelSkipgramCommands.save_model('result/modelSkipgramCommands.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelSkipgram = fasttext.load_model(\"result/modelSkipgram.bin\")\n",
    "modelSkipgramFull = fasttext.load_model(\"result/modelSkipgramFull.bin\")\n",
    "modelSkipgramCommands = fasttext.load_model('result/modelSkipgramCommands.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fasttext to tensorflow\n",
    "\n",
    "# Getting the vocabulary and embedding dimension\n",
    "words = modelSkipgramFull.get_words()\n",
    "embedding_dim = modelSkipgramFull.get_dimension()\n",
    "\n",
    "\n",
    "# Dictionary mapping words to their indices\n",
    "word_index = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "embedding_matrix = np.zeros((len(words), embedding_dim))\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = modelSkipgramFull.get_word_vector(word)\n",
    "\n",
    "\n",
    "# TensorFlow embedding layer\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(words),\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "def get_word_embedding(word):\n",
    "    word_id = word_index.get(word)\n",
    "    if word_id is None:\n",
    "        raise ValueError(f\"Word '{word}' not in vocabulary.\")\n",
    "    return embedding_layer(tf.constant([word_id]))[0].numpy()\n",
    "'''\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    if word in word_index:\n",
    "        # If the word is in vocabulary, use the pre-trained embedding\n",
    "        word_id = word_index[word]\n",
    "        return embedding_layer(tf.constant([word_id]))[0].numpy()\n",
    "    else:\n",
    "        # For OOV words, use FastText's subword information\n",
    "        return modelSkipgramFull.get_word_vector(word)\n",
    "\n",
    "def get_multiple_embeddings(words):\n",
    "    word_ids = [word_index[word] for word in words if word in word_index]\n",
    "    return embedding_layer(tf.constant(word_ids)).numpy()\n",
    "\n",
    "\n",
    "def most_similar_word(input_word):\n",
    "    # Get the embedding of the input word\n",
    "    input_vector = get_word_embedding(input_word)\n",
    "    \n",
    "    input_vector = input_vector.reshape(1, -1)\n",
    "\n",
    "    max_similarity = -1\n",
    "    most_similar = None\n",
    "\n",
    "    # Iterate through the vocabulary\n",
    "    for word, idx in word_index.items():\n",
    "        # Get the embedding vector of the current word\n",
    "        word_vector = embedding_layer(tf.constant([idx]))[0].numpy()\n",
    "\n",
    "        word_vector = word_vector.reshape(1, -1)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(input_vector, word_vector)[0][0]\n",
    "        \n",
    "        # Update the most similar word if this one has higher similarity\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar = word\n",
    "    \n",
    "    return most_similar, max_similarity\n",
    "\n",
    "# similar_word, similarity_score = most_similar_word(\"bot\")\n",
    "# print(f\"The most similar word to 'example' is '{similar_word}' with a cosine similarity of {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar word is 'job' with a cosine similarity of 0.3667\n"
     ]
    }
   ],
   "source": [
    "def most_similar_word_optimized(input_word):\n",
    "    # Get the embedding of the input word\n",
    "    input_vector = get_word_embedding(input_word)\n",
    "    \n",
    "    # Normalize the input vector\n",
    "    input_vector = input_vector / norm(input_vector)\n",
    "    \n",
    "    # Extract all word embeddings in the vocabulary from the embedding layer\n",
    "    embedding_matrix = embedding_layer.weights[0].numpy()\n",
    "\n",
    "    # Normalize the embedding matrix row-wise\n",
    "    embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute cosine similarities as a dot product between input vector and all embeddings\n",
    "    similarities = np.dot(embedding_matrix, input_vector)\n",
    "    \n",
    "    # Find the index of the maximum similarity\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    most_similar_word = list(word_index.keys())[most_similar_idx]\n",
    "    max_similarity = similarities[most_similar_idx]\n",
    "\n",
    "    return most_similar_word, max_similarity\n",
    "\n",
    "# Example usage\n",
    "similar_word, similarity_score = most_similar_word_optimized(\"mkdir\")\n",
    "print(f\"The most similar word is '{similar_word}' with a cosine similarity of {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6302575\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity between word vectors\n",
    "\n",
    "# Word embeddings for two words\n",
    "word1_embedding = get_word_embedding(\"mkdir\")\n",
    "word2_embedding = get_word_embedding(\"mikdr\")\n",
    "\n",
    "# Reshape the arrays to match the expected input shape of cosine_similarity\n",
    "word1_embedding = word1_embedding.reshape(1, -1)\n",
    "word2_embedding = word2_embedding.reshape(1, -1)\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity(word1_embedding, word2_embedding)[0][0]\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fasttext to tensorflow\n",
    "\n",
    "# Getting the vocabulary and embedding dimension\n",
    "words = modelSkipgramCommands.get_words()\n",
    "embedding_dim = modelSkipgramCommands.get_dimension()\n",
    "\n",
    "\n",
    "# Dictionary mapping words to their indices\n",
    "word_index = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "embedding_matrix = np.zeros((len(words), embedding_dim))\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = modelSkipgramCommands.get_word_vector(word)\n",
    "\n",
    "\n",
    "# TensorFlow embedding layer\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(words),\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "def get_word_embedding(word):\n",
    "    word_id = word_index.get(word)\n",
    "    if word_id is None:\n",
    "        raise ValueError(f\"Word '{word}' not in vocabulary.\")\n",
    "    return embedding_layer(tf.constant([word_id]))[0].numpy()\n",
    "'''\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    if word in word_index:\n",
    "        # If the word is in vocabulary, use the pre-trained embedding\n",
    "        word_id = word_index[word]\n",
    "        return embedding_layer(tf.constant([word_id]))[0].numpy()\n",
    "    else:\n",
    "        # For OOV words, use FastText's subword information\n",
    "        return modelSkipgramCommands.get_word_vector(word)\n",
    "\n",
    "def get_multiple_embeddings(words):\n",
    "    word_ids = [word_index[word] for word in words if word in word_index]\n",
    "    return embedding_layer(tf.constant(word_ids)).numpy()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
